{"cells":[{"cell_type":"markdown","metadata":{"id":"-y3S2vgqJi8t"},"source":["# 부스팅\n","\n","- 머신러닝에서 부스팅은 오차를 줄이기 위해 사용되는 학습 방법. 앙상블 기법 중 하나로 부스팅은 이전 모델들이 만든 오차에 집중하여 그 오차를 보완하는 새로운 모델을 반복적으로 학습시킴"]},{"cell_type":"markdown","metadata":{"id":"f8o6YL6vbqrH"},"source":["## 부스팅의 주요 단계\n","1. (Base Model 학습): 첫 번째 모델은 전체 데이터셋을 사용하여 학습된다. 이 모델은 예측에서 오차를 만들어냄.\n","\n","2. (Error에 집중한 모델 학습): 첫 번째 모델이 만든 오차에 더 집중하는 두 번째 모델을 학습한다. 두 번째 모델은 오차에 가중치를 부여하여 예측 모델을 보완\n","\n","3. (반복): 이후에도 계속해서 오차에 집중하는 모델을 학습하고 각 모델은 이전 모델들이 만든 오차를 보완하며, 전체 앙상블은 강력한 예측 성능을 갖게 됨"]},{"cell_type":"markdown","metadata":{"id":"7GKbsdqEb6ia"},"source":["## 부스팅의 특징\n","- 가중치 부여: 각 모델은 이전 모델들이 만든 오차에 가중치를 부여하여 학습된다.\n","\n","- Adaptive Boosting (AdaBoost), Gradient Boosting (GBM), XGBoost, LightGBM, CatBoost 등 여러 부스팅 알고리즘이 있다."]},{"cell_type":"markdown","metadata":{"id":"wBo_Jvm1cALG"},"source":["## 부스팅의 장점\n","- 높은 성능: 일반적으로 부스팅은 높은 예측 성능을 제공합니다.\n","\n","- Overfitting 방지: 오차에 집중하여 학습하므로 과적합을 방지하는 효과가 있습니다.\n","\n","- 다양한 데이터 패턴 학습: 각 모델이 이전 모델의 오차를 보완하면서 다양한 데이터 패턴을 학습합니다."]},{"cell_type":"markdown","metadata":{"id":"aAaFOqq_cHUz"},"source":["**부스팅 예시:**\n","\n","1. **첫 번째 모델 (Decision Tree):**\n","   - 전체 데이터셋을 사용하여 학습\n","\n","2. **두 번째 모델 (Decision Tree):**\n","   - 첫 번째 모델이 만든 오차에 가중치를 부여하여 학습\n","\n","3. **세 번째 모델 (Decision Tree):**\n","   - 두 번째 모델이 만든 오차에 가중치를 부여하여 학습\n","\n","4. **반복...**"]},{"cell_type":"markdown","metadata":{"id":"V9jbYxlRKvzh"},"source":["### 예제"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fQU6T6CiKxtc"},"outputs":[],"source":["from sklearn.ensemble import AdaBoostClassifier\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# 데이터 생성\n","X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n","\n","# 데이터 분할\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# AdaBoost 모델 생성\n","adaboost_model = AdaBoostClassifier(n_estimators=50, random_state=42)\n","\n","# 모델 훈련\n","adaboost_model.fit(X_train, y_train)\n","\n","# 예측\n","y_pred = adaboost_model.predict(X_test)\n","\n","# 정확도 평가\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f'AdaBoost 정확도: {accuracy:.4f}')"]},{"cell_type":"markdown","metadata":{"id":"4oHoR9WWUvhH"},"source":["# 스태킹\n","- 스태킹(Stacking)은 여러 머신러닝 모델의 예측 결과를 결합하여 높은 성능의 모델을 만들기 위한 앙상블(Ensemble) 기법 중 하나이다. 스태킹은 여러 모델이 만들어낸 예측 값을 취합하여 이를 다시 하나의 메타 모델에 입력으로 사용하는 방식으로 동작\n"]},{"cell_type":"markdown","metadata":{"id":"Clk3OtkHa6QC"},"source":["## 스태킹의 주요 단계\n","1. (Base Models): 다양한 머신러닝 모델들을 사용하여 데이터를 학습시키고 각 모델은 독립 변수를 사용하여 종속 변수를 예측\n","\n","2. (Meta Model): 첫 번째 단계에서 생성된 예측 값을 모은 후, 이를 독립 변수로 사용하여 하나의 메타 모델(또는 앙상블 모델)을 학습시킨다. 이 메타 모델은 개별 모델들의 예측을 토대로 최종 예측을 수행하는 역할함\n","\n","3. 최종 예측: 학습된 메타 모델을 사용하여 새로운 데이터에 대한 최종 예측을 수행"]},{"cell_type":"markdown","metadata":{"id":"A-Vpi3wubIWC"},"source":["## 스태킹의 장점\n","- 고수준의 성능: 다양한 모델의 다양성을 활용하여 보다 뛰어난 성능을 달성할 수 있습니다.\n","\n","- 적응력: 다양한 유형의 모델을 함께 사용함으로써 다양한 데이터 패턴에 대응할 수 있습니다.\n","\n","- 일반화 성능 향상: 과적합을 방지하고 일반화 성능을 향상시킬 수 있습니다."]},{"cell_type":"markdown","metadata":{"id":"MsaolEGSbQca"},"source":["**스태킹 예시:**\n","\n","1. **첫 번째 단계 (Base Models):**\n","   - 결정 트리\n","   - 랜덤 포레스트\n","   - 서포트 벡터 머신\n","\n","2. **두 번째 단계 (Meta Model):**\n","   - 로지스틱 회귀\n","\n","3. **최종 예측:**\n","   - 각 모델의 예측을 모은 후 로지스틱 회귀로 최종 예측 수행"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.ensemble import StackingClassifier\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# 데이터 생성\n","X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n","\n","# 데이터 분할\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# 기본 모델 정의\n","base_models = [\n","    ('svm', SVC(probability=True)),\n","    ('random_forest', RandomForestClassifier(n_estimators=50, random_state=42)),\n","    ('knn', KNeighborsClassifier(n_neighbors=5))\n","]\n","\n","# 최종 모델 정의\n","final_model = LogisticRegression()\n","\n","# 스태킹 모델 생성\n","stacking_model = StackingClassifier(estimators=base_models, final_estimator=final_model)\n","\n","# 모델 훈련\n","stacking_model.fit(X_train, y_train)\n","\n","# 예측\n","y_pred = stacking_model.predict(X_test)\n","\n","# 정확도 평가\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f'Stacking 정확도: {accuracy:.4f}')"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMloZbznIJPcUQzV2A4zAkq","mount_file_id":"155xstkBPqJicCUk6A-vNU9vwyIUBPrJP","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
